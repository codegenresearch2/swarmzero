from unittest.mock import MagicMock, patch\\\\\nimport pytest\\\\\nfrom llama_index.agent.openai import OpenAIAgent  # type: ignore\\\\\nfrom llama_index.core.llms import ChatMessage, MessageRole\\\\\nfrom llama_index.multi_modal_llms.openai import OpenAIMultiModal  # type: ignore\\\\\nfrom swarmzero.chat import ChatManager\\\\\n\\\\nclass MockAgent:\\\\\n    async def astream_chat(self, content, chat_history=None):\\\\\n        async def async_response_gen():\\\\\n            yield 'chat response'\\\\\n\\\\n        return type('MockResponse', (), {'async_response_gen': async_response_gen})\\\\\n\\\\n    async def achat(self, content, chat_history=None):\\\\\n        return type('MockResponse', (), {'response': 'chat response'})\\\\n\\\\nclass MockMultiModalAgent:\\\\\n    def create_task(self, content, extra_state=None):\\\\\n        return type('MockTask', (), {'task_id': '12345'})\\\\n\\\\n    async def _arun_step(self, task_id):\\\\\n        return type('MockResponse', (), {'is_last': True})\\\\n\\\\n    def finalize_response(self, task_id):\\\\\n        return 'multimodal response'\\\\n\\\\nclass MockDatabaseManager:\\\\\n    def __init__(self):\\\\\n        self.data = []\\\\n\\\\n    async def insert_data(self, table_name: str, data: dict):\\\\\n        self.data.append(data)\\\\\n\\\\n    async def read_data(self, table_name: str, filters: dict):\\\\\n        return [d for d in self.data if all(d[k] == v[0] for k, v in filters.items())]\\\\\n\\\\n@pytest.fixture\\\\ndef agent():\\\\n    return MockAgent()\\\\n\\\\n@pytest.fixture\\\\ndef multi_modal_agent():\\\\n    agent = MockMultiModalAgent()\\\\n    agent._arun_step = MagicMock(side_effect=agent._arun_step) \\\\n    agent.finalize_response = MagicMock(side_effect=agent.finalize_response) \\\\n    return agent\\\\n\\\\n@pytest.fixture\\\\ndef db_manager():\\\\n    return MockDatabaseManager()\\\\n\\\\n@pytest.mark.asyncio\\\\nasync def test_add_message(agent, db_manager):\\\\n    chat_manager = ChatManager(agent, user_id='123', session_id='abc') \\\\n    await chat_manager.add_message(db_manager, MessageRole.USER, 'Hello!') \\\\n    messages = await chat_manager.get_messages(db_manager) \\\\n    assert len(messages) == 1 \\\\n    assert messages[0].content == 'Hello!'\\\\n\\\\n@pytest.mark.asyncio\\\\nasync def test_generate_response_with_generic_llm(agent, db_manager):\\\\n    chat_manager = ChatManager(agent, user_id='123', session_id='abc') \\\\n    user_message = ChatMessage(role=MessageRole.USER, content='Hello!') \\\\n\\\\n    response = await chat_manager.generate_response(db_manager, user_message, []) \\\\n    assert response == 'chat response' \\\\n\\\\n    messages = await chat_manager.get_messages(db_manager) \\\\n    assert len(messages) == 2 \\\\n    assert messages[0].content == 'Hello!' \\\\n    assert messages[1].content == 'chat response'\\\\n\\\\n@pytest.mark.asyncio\\\\nasync def test_get_all_chats_for_user(agent, db_manager):\\\\n    chat_manager1 = ChatManager(agent, user_id='123', session_id='abc') \\\\n    await chat_manager1.add_message(db_manager, MessageRole.USER, 'Hello in abc') \\\\n    await chat_manager1.add_message(db_manager, MessageRole.ASSISTANT, 'Response in abc') \\\\n\\\\n    chat_manager2 = ChatManager(agent, user_id='123', session_id='def') \\\\n    await chat_manager2.add_message(db_manager, MessageRole.USER, 'Hello in def') \\\\n    await chat_manager2.add_message(db_manager, MessageRole.ASSISTANT, 'Response in def') \\\\n\\\\n    chat_manager = ChatManager(agent, user_id='123', session_id='') \\\\n    all_chats = await chat_manager.get_all_chats_for_user(db_manager) \\\\n\\\\n    assert 'abc' in all_chats \\\\n    assert 'def' in all_chats \\\\n\\\\n    assert len(all_chats['abc']) == 2 \\\\n    assert all_chats['abc'][0]['message'] == 'Hello in abc' \\\\n    assert all_chats['abc'][1]['message'] == 'Response in abc' \\\\n\\\\n    assert len(all_chats['def']) == 2 \\\\n    assert all_chats['def'][0]['message'] == 'Hello in def' \\\\n    assert all_chats['def'][1]['message'] == 'Response in def'\\\\n\\\\n@pytest.mark.asyncio\\\\nasync def test_generate_response_with_openai_multimodal(multi_modal_agent, db_manager):\\\\n    with patch('llama_index.core.settings._Settings.llm', new=MagicMock(spec=OpenAIMultiModal)):\\\\n        chat_manager = ChatManager(multi_modal_agent, user_id='123', session_id='abc', enable_multi_modal=True) \\\\n        user_message = ChatMessage(role=MessageRole.USER, content='Hello!') \\\\n        files = ['image1.png', 'image2.png'] \\\\n\\\\n        response = await chat_manager.generate_response(db_manager, user_message, files) \\\\n\\\\n        assert response == 'multimodal response' \\\\n\\\\n        messages = await chat_manager.get_messages(db_manager) \\\\n        assert len(messages) == 2 \\\\n        assert messages[0].content == 'Hello!' \\\\n        assert messages[1].content == 'multimodal response'\\\\n\\\\n@pytest.mark.asyncio\\\\nasync def test_execute_task_success(multi_modal_agent):\\\\n    chat_manager = ChatManager(multi_modal_agent, user_id='123', session_id='abc') \\\\n\\\\n    result = await chat_manager._execute_task('task_id_123') \\\\n\\\\n    assert result == 'multimodal response' \\\\n    multi_modal_agent._arun_step.assert_called_once_with('task_id_123') \\\\n    multi_modal_agent.finalize_response.assert_called_once_with('task_id_123') \\\\n\\\\n@pytest.mark.asyncio\\\\nasync def test_execute_task_with_exception(multi_modal_agent):\\\\n    async def mock_arun_step(task_id):\\\\n        raise ValueError(f'Could not find step_id: {task_id}') \\\\n\\\\n    multi_modal_agent._arun_step = MagicMock(side_effect=mock_arun_step) \\\\n\\\\n    chat_manager = ChatManager(multi_modal_agent, user_id='123', session_id='abc') \\\\n\\\\n    result = await chat_manager._execute_task('task_id_123') \\\\n\\\\n    assert result == 'error during step execution: Could not find step_id: task_id_123' \\\\n    multi_modal_agent._arun_step.assert_called_once_with('task_id_123') \\\\n\\\\n@pytest.mark.asyncio\\\\nasync def test_generate_response_with_openai_agent(agent, db_manager):\\\\n    with patch('llama_index.core.settings._Settings.llm', new=MagicMock(spec=OpenAIAgent)):\\\\n        chat_manager = ChatManager(agent, user_id='123', session_id='abc') \\\\n        user_message = ChatMessage(role=MessageRole.USER, content='Hello!') \\\\n\\\\n        response = await chat_manager.generate_response(db_manager, user_message, []) \\\\n        assert response == 'chat response' \\\\n\\\\n        messages = await chat_manager.get_messages(db_manager) \\\\n        assert len(messages) == 2 \\\\n        assert messages[0].content == 'Hello!' \\\\n        assert messages[1].content == 'chat response'\\\\n